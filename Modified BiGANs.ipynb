{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a decoder network - with input as generated images that outputs the input random variable in the GAN. Add a norm-based reconstruction loss between the input to the generator and the output of the decoder. Train it simultaneously along with regular GAN losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize, Resize\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torchinfo import summary\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataroot = \"data/Animals_data/animals/animals\"\n",
    "TRAIN_DCGAN = True\n",
    "N_EPOCHS = 250\n",
    "BATCH_SIZE = 64\n",
    "N_critic = 1\n",
    "z_dim = 100\n",
    "Img_channels = 3\n",
    "Input_Shape = (3, 128, 128)\n",
    "Hidden_dims = 64\n",
    "lr = 1e-4\n",
    "betas = (0.5, 0.999)\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "NUM_WORKERS = 1  # Number of dataloader workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard stuff\n",
    "\n",
    "dcgan_writer = SummaryWriter(log_dir=f'DC_GAN/tensorboard')\n",
    "\n",
    "\n",
    "def log_losses_to_tensorboard(epoch, g_loss, d_loss, dec_loss):\n",
    "    dcgan_writer.add_scalar('Loss/Generator', g_loss, epoch)\n",
    "    dcgan_writer.add_scalar('Loss/Discriminator', d_loss, epoch)\n",
    "    dcgan_writer.add_scalar('Loss/Decoder', dec_loss, epoch)\n",
    "\n",
    "\n",
    "def log_gradients_to_tensorboard(writer, model, epoch, model_name):\n",
    "    total_norm = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            norm = param.grad.norm(2).item()\n",
    "            total_norm += norm ** 2\n",
    "            writer.add_scalar(f'Gradients/{model_name}/{name}', norm, epoch)\n",
    "    total_norm = total_norm ** 0.5\n",
    "    writer.add_scalar(f'Gradients/{model_name}/total_norm', total_norm, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, Input_channels=Img_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=Input_channels, out_channels=Hidden_dims,\n",
    "                      kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm2d(Hidden_dims),\n",
    "            nn.Conv2d(in_channels=Hidden_dims, out_channels=Hidden_dims,\n",
    "                      kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm2d(Hidden_dims),\n",
    "            nn.Conv2d(in_channels=Hidden_dims, out_channels=2*Hidden_dims,\n",
    "                      kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm2d(2*Hidden_dims),\n",
    "            nn.Conv2d(in_channels=2*Hidden_dims, out_channels=4 *\n",
    "                      Hidden_dims, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm2d(4*Hidden_dims),\n",
    "            nn.Conv2d(in_channels=4*Hidden_dims, out_channels=8 *\n",
    "                      Hidden_dims, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm2d(8*Hidden_dims),\n",
    "            nn.Conv2d(in_channels=8*Hidden_dims, out_channels=1,\n",
    "                      kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator architecture\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z=z_dim, Output_channels=Img_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=z_dim, out_channels=4*Hidden_dims,\n",
    "                               # [4x4]\n",
    "                               kernel_size=7, stride=1, padding=1, bias=False),\n",
    "            nn.LayerNorm([4*Hidden_dims, 5, 5]),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.UpsamplingNearest2d(\n",
    "                scale_factor=2),  # [8x8]\n",
    "            nn.Conv2d(in_channels=4*Hidden_dims, out_channels=2 * \\\n",
    "                      Hidden_dims, kernel_size=7, stride=1, padding=2),\n",
    "            nn.LayerNorm([2*Hidden_dims, 8, 8]),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.UpsamplingNearest2d(\n",
    "                scale_factor=2),  # [16x16]\n",
    "            nn.Conv2d(in_channels=2*Hidden_dims, out_channels=Hidden_dims,\n",
    "                      kernel_size=5, stride=1, padding=2),\n",
    "            nn.LayerNorm([Hidden_dims, 16, 16]),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.UpsamplingNearest2d(\n",
    "                scale_factor=2),  # [32x32]\n",
    "            nn.Conv2d(in_channels=Hidden_dims, out_channels=Hidden_dims,\n",
    "                      kernel_size=5, stride=1, padding=2),\n",
    "            nn.LayerNorm([Hidden_dims, 32, 32]),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.UpsamplingNearest2d(\n",
    "                scale_factor=2),  # [64x64]\n",
    "            nn.Conv2d(in_channels=Hidden_dims, out_channels=Hidden_dims,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.LayerNorm([Hidden_dims, 64, 64]),\n",
    "            nn.LeakyReLU(0.5, True),\n",
    "\n",
    "            nn.UpsamplingNearest2d(\n",
    "                scale_factor=2),  # [128x128]\n",
    "            nn.Conv2d(in_channels=Hidden_dims, out_channels=Output_channels,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "weights_init(generator)\n",
    "weights_init(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(discriminator,input_size=(1, 3,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(generator,input_size=(1, 100,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_noise(size=z_dim, batch_size=BATCH_SIZE):\n",
    "    return torch.randn(batch_size, size, 1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gen = torch.optim.Adam(generator.parameters(), lr=0.0001, betas=betas)\n",
    "opt_disc = torch.optim.Adam(\n",
    "    discriminator.parameters(), lr=0.00012, betas=betas)\n",
    "scheduler_gen = torch.optim.lr_scheduler.StepLR(\n",
    "    opt_gen, step_size=N_EPOCHS//4, gamma=0.5)\n",
    "scheduler_disc = torch.optim.lr_scheduler.StepLR(\n",
    "    opt_disc, step_size=N_EPOCHS//4, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "])\n",
    "img_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=dataroot, transform=transform)\n",
    "dataloader = DataLoader(img_dataset, num_workers=NUM_WORKERS,\n",
    "                        batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, img_channels=3, z_dim=z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=img_channels, out_channels=Hidden_dims,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.LayerNorm([Hidden_dims, 128, 128]),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.AvgPool2d(kernel_size=2),  # [64x64]\n",
    "            nn.Conv2d(in_channels=Hidden_dims, out_channels=Hidden_dims,\n",
    "                      kernel_size=3, stride=1, padding=1),  # Keep spatial size\n",
    "            nn.LayerNorm([Hidden_dims, 64, 64]),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.AvgPool2d(kernel_size=2),  # [32x32]\n",
    "            nn.Conv2d(in_channels=Hidden_dims, out_channels=Hidden_dims,\n",
    "                      kernel_size=5, stride=1, padding=2),  # Keep spatial size\n",
    "            nn.LayerNorm([Hidden_dims, 32, 32]),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.AvgPool2d(kernel_size=2),  # [16x16]\n",
    "            nn.Conv2d(in_channels=Hidden_dims, out_channels=2*Hidden_dims,\n",
    "                      kernel_size=5, stride=1, padding=2),  # Keep spatial size\n",
    "            nn.LayerNorm([2*Hidden_dims, 16, 16]),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.AvgPool2d(kernel_size=2),  # [8x8]\n",
    "            nn.Conv2d(in_channels=2*Hidden_dims, out_channels=4*Hidden_dims,\n",
    "                      kernel_size=5, stride=1, padding=2),  # Keep spatial size\n",
    "            nn.LayerNorm([4*Hidden_dims, 8, 8]),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.AvgPool2d(kernel_size=2),  # [4x4]\n",
    "            nn.Conv2d(in_channels=4*Hidden_dims, out_channels=z_dim,\n",
    "                      kernel_size=4, stride=1, padding=0),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(z_dim=z_dim).to(device)\n",
    "weights_init(decoder)\n",
    "summary(decoder,input_size=(1, 3,128,128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our decoder loss we use a combination:\n",
    "1. A L2 norm-based reconstruction loss between Z and Decoder(Generator(Z))\n",
    "2. A L1 perceptual loss between real images and Generator(Decoder(real images)). We prefer L1 over MSE to reduce sensitivty to outliers when comparing pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_dec = torch.optim.Adam(\n",
    "    decoder.parameters(), lr=0.0001, weight_decay=0.0001, betas=betas)\n",
    "scheduler_dec = torch.optim.lr_scheduler.StepLR(\n",
    "    opt_dec, step_size=N_EPOCHS//4, gamma=0.5)\n",
    "criterion_recon = nn.MSELoss()  # Norm-based reconstruction loss\n",
    "perceptual_loss = nn.L1Loss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiable Augmentation for Data-Efficient GAN Training\n",
    "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
    "# https://arxiv.org/pdf/2006.10738\n",
    "# Code take from: https://github.com/mit-han-lab/data-efficient-gans/blob/master/DiffAugment_pytorch.py\n",
    "\n",
    "def DiffAugment(x, policy='color,translation,cutout', channels_first=True):\n",
    "    if policy:\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        for p in policy.split(','):\n",
    "            for f in AUGMENT_FNS[p]:\n",
    "                x = f(x)\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "        x = x.contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_brightness(x):\n",
    "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_saturation(x):\n",
    "    x_mean = x.mean(dim=1, keepdim=True)\n",
    "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1,\n",
    "                                   dtype=x.dtype, device=x.device) * 2) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_contrast(x):\n",
    "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
    "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1,\n",
    "                                   dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_translation(x, ratio=0.125):\n",
    "    shift_x, shift_y = int(x.size(2) * ratio +\n",
    "                           0.5), int(x.size(3) * ratio + 0.5)\n",
    "    translation_x = torch.randint(-shift_x, shift_x + 1,\n",
    "                                  size=[x.size(0), 1, 1], device=x.device)\n",
    "    translation_y = torch.randint(-shift_y, shift_y + 1,\n",
    "                                  size=[x.size(0), 1, 1], device=x.device)\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
    "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
    "    x_pad = torch.nn.functional.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
    "    x = x_pad.permute(0, 2, 3, 1).contiguous()[\n",
    "        grid_batch, grid_x, grid_y].permute(0, 3, 1, 2).contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_cutout(x, ratio=0.5):\n",
    "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    offset_x = torch.randint(0, x.size(\n",
    "        2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "    offset_y = torch.randint(0, x.size(\n",
    "        3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + offset_x -\n",
    "                         cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
    "    grid_y = torch.clamp(grid_y + offset_y -\n",
    "                         cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
    "    mask = torch.ones(x.size(0), x.size(2), x.size(3),\n",
    "                      dtype=x.dtype, device=x.device)\n",
    "    mask[grid_batch, grid_x, grid_y] = 0\n",
    "    x = x * mask.unsqueeze(1)\n",
    "    return x\n",
    "\n",
    "\n",
    "AUGMENT_FNS = {\n",
    "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
    "    'translation': [rand_translation],\n",
    "    'cutout': [rand_cutout],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "def train_dcgan():\n",
    "    step = 0\n",
    "    EPS = 1e-9\n",
    "    for epoch in trange(N_EPOCHS):\n",
    "        for (X, _) in dataloader:\n",
    "            X = X.to(device)\n",
    "            curr_batch_size = X.shape[0]\n",
    "            # X = X + torch.rand(size = X.shape )/5\n",
    "\n",
    "            # training discriminator\n",
    "            for _ in range(N_critic):\n",
    "                # real images\n",
    "                opt_disc.zero_grad()\n",
    "                d_real = discriminator(DiffAugment(X))\n",
    "                loss_d_real = -1*torch.log(d_real+EPS).mean()\n",
    "                loss_d_real.backward(retain_graph=True)\n",
    "                # fake images\n",
    "                noise = sample_noise(batch_size=curr_batch_size)\n",
    "                fake = generator(noise)\n",
    "                d_fake = discriminator(DiffAugment(fake).detach())\n",
    "                loss_d_fake = -1*torch.log(1-d_fake+EPS).mean()\n",
    "                loss_d_fake.backward(retain_graph=True)\n",
    "\n",
    "                opt_disc.step()\n",
    "\n",
    "            # training generator\n",
    "            opt_gen.zero_grad()\n",
    "\n",
    "            g_fake = discriminator(DiffAugment(fake))\n",
    "            loss_g = (-1*torch.log(g_fake+EPS)).mean()\n",
    "\n",
    "            opt_dec.zero_grad()\n",
    "            reconstructed_noise = decoder(DiffAugment(fake))\n",
    "            loss_recon = criterion_recon(reconstructed_noise, noise)\n",
    "            aug_real = DiffAugment(X)\n",
    "            real_image_noise = decoder(aug_real)\n",
    "            image_recon_loss = perceptual_loss(\n",
    "                generator(real_image_noise), aug_real)\n",
    "\n",
    "            total_loss = loss_g + loss_recon + image_recon_loss\n",
    "            total_loss.backward(retain_graph=True)\n",
    "\n",
    "            opt_dec.step()\n",
    "            opt_gen.step()\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                print(f\"epoch:{\n",
    "                    epoch+1} iter{step} disc_loss:{(loss_d_fake+loss_d_real)} gen_loss:{loss_g} dec_loss:{(loss_recon.item())}\")\n",
    "\n",
    "            if step % 200 == 0:\n",
    "                with torch.no_grad():\n",
    "                    # Use the first image from fake_images instead of generating new ones\n",
    "                    # Take the first image and add batch dimension\n",
    "                    fake_images = fake[0].unsqueeze(0).cpu()\n",
    "                    # Ensure the image is in the range [0, 1]\n",
    "                    fake = (fake + 1) / 2.0  # Transform from [-1, 1] to [0, 1]\n",
    "                    img = torchvision.utils.make_grid(fake, normalize=False)\n",
    "                    img_np = img.cpu().detach().permute(1, 2, 0).numpy()  # Add detach() here\n",
    "                    plt.figure(figsize=(8, 8))\n",
    "                    plt.imshow(img_np)\n",
    "                    plt.axis('off')\n",
    "                    plt.title(f\"Epoch {epoch+1}, iter {step}\")\n",
    "                    save_dir = \"DC_GAN/generated_images\"\n",
    "                    os.makedirs(save_dir, exist_ok=True)\n",
    "                    save_path = os.path.join(save_dir, f'generated_image_epoch_{\n",
    "                        epoch+1}_batch_{step+1}.png')\n",
    "                    plt.savefig(save_path)\n",
    "                    plt.close()\n",
    "            step += 1\n",
    "\n",
    "        scheduler_dec.step()\n",
    "        scheduler_disc.step()\n",
    "        scheduler_gen.step()\n",
    "\n",
    "        log_losses_to_tensorboard(epoch, loss_g.item(\n",
    "        ), loss_d_fake.item()+loss_d_real.item(), loss_recon.item())\n",
    "        log_gradients_to_tensorboard(\n",
    "            dcgan_writer, generator, epoch, 'Generator')\n",
    "        log_gradients_to_tensorboard(\n",
    "            dcgan_writer, discriminator, epoch, 'Discriminator')\n",
    "        log_gradients_to_tensorboard(dcgan_writer, decoder, epoch, 'Decoder')\n",
    "\n",
    "\n",
    "save_dir = \"DC_GAN/models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path_g = os.path.join(save_dir, \"generator\")\n",
    "save_path_d = os.path.join(save_dir, \"discriminator\")\n",
    "save_path_dec = os.path.join(save_dir, \"decoder\")\n",
    "\n",
    "\n",
    "if TRAIN_DCGAN:\n",
    "    train_dcgan()\n",
    "    torch.save(generator.state_dict(), save_path_g)\n",
    "    torch.save(discriminator.state_dict(), save_path_d)\n",
    "    torch.save(decoder.state_dict(), save_path_dec)\n",
    "else:\n",
    "    decoder.load_state_dict(torch.load(save_path_dec, weights_only=True))\n",
    "    decoder.eval()\n",
    "    generator.load_state_dict(torch.load(save_path_g, weights_only=True))\n",
    "    generator.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the decoder output (trained in the previous step) for all the input\n",
    "images. Train an MLP to solve a classification task by taking these decoded vectors as input. Compute and report the classification accuracy\n",
    "and the F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a t-SNE plot of the decoded latents for all real images to check if they are separable by class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "z_vectors = []\n",
    "class_labels = []\n",
    "\n",
    "with torch.no_grad():  # No need to compute gradients for this\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        z = decoder(images).squeeze(-1).squeeze(-1)  # Get the latent vector z\n",
    "        z_vectors.append(z)\n",
    "        class_labels.append(labels)\n",
    "\n",
    "# Convert lists to tensors and numpy arrays\n",
    "# Shape (num_samples, latent_dim)\n",
    "z_vectors = torch.cat(z_vectors).cpu().numpy()\n",
    "class_labels = torch.cat(class_labels).cpu().numpy()  # Shape (num_samples,)\n",
    "\n",
    "# Apply t-SNE to reduce the latent vectors to 2D\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "z_tsne = tsne.fit_transform(z_vectors)\n",
    "\n",
    "# Plot the t-SNE results\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Using 'tab10' for 10 MNIST classes\n",
    "scatter = plt.scatter(z_tsne[:, 0], z_tsne[:, 1],\n",
    "                      c=class_labels, cmap='tab10', s=50)\n",
    "plt.colorbar(scatter)\n",
    "plt.title(\"t-SNE of Latent Vectors with Class Labels\")\n",
    "plt.xlabel(\"t-SNE component 1\")\n",
    "plt.ylabel(\"t-SNE component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuraion for MLP and resnet\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x.squeeze(-1).squeeze(-1)))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_datasets(img_dataset):\n",
    "    train_size = int(0.8 * len(img_dataset))\n",
    "    test_size = len(img_dataset) - train_size\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    train_dataset, test_dataset = random_split(\n",
    "        img_dataset, [train_size, test_size])\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "test_dataset, train_dataset = prepare_datasets(img_dataset)\n",
    "train_loader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(model, num_classes, test_loader, fn=lambda img: img):\n",
    "    model.eval()\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "    true_positives = torch.zeros(num_classes).to(device)\n",
    "    false_positives = torch.zeros(num_classes).to(device)\n",
    "    false_negatives = torch.zeros(num_classes).to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for data in test_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(fn(images))\n",
    "            _, predicted = outputs.topk(\n",
    "                5, 1, largest=True, sorted=True)\n",
    "            predicted = predicted.t()\n",
    "            top1_correct += (predicted[0] == labels).sum().item()\n",
    "            top5_correct += (predicted ==\n",
    "                             labels.unsqueeze(0)).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                pred = predicted[0][i]\n",
    "                if pred == label:\n",
    "                    true_positives[label] += 1\n",
    "                else:\n",
    "                    false_positives[pred] += 1\n",
    "                    false_negatives[label] += 1\n",
    "\n",
    "    # Calculate precision, recall, and F1 for each class\n",
    "    precision_per_class = true_positives / \\\n",
    "        (true_positives + false_positives + 1e-10)  # Avoid division by zero\n",
    "    recall_per_class = true_positives / \\\n",
    "        (true_positives + false_negatives + 1e-10)\n",
    "\n",
    "    # Average precision and recall over all classes\n",
    "    precision = precision_per_class.mean().item()\n",
    "    recall = recall_per_class.mean().item()\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "\n",
    "    top1_accuracy = 100 * top1_correct / total\n",
    "    top5_accuracy = 100 * top5_correct / total\n",
    "    return top1_accuracy, top5_accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(img_dataset.classes)\n",
    "mlp = MLP(z_dim, 128, len(img_dataset.classes)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "mlp_optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in trange(EPOCHS):\n",
    "    mlp.train()\n",
    "    for (X, y) in train_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        mlp_optimizer.zero_grad()\n",
    "        noise = decoder(X)\n",
    "        outputs = mlp(noise.detach())\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        mlp_optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        top1_accuracy, top5_accuracy, f1 = test_classifier(\n",
    "            model=mlp, num_classes=num_classes, test_loader=test_loader, fn=lambda image: decoder(image))\n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f} Top1: {top1_accuracy} Top5: {top5_accuracy}, F1: {f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ForADRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
